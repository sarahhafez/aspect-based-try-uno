# -*- coding: utf-8 -*-
"""Data Set Analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18Ir5YS0EvbyBl-q8ClDqu0x9cinXEz2U

##**Importing libraries and the data**
"""

!pip install googletrans

import json
import csv
import random
import nltk
from googletrans import Translator 
import regex as re
from collections import Counter
from sklearn.svm import LinearSVC
from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics
from nltk.corpus import stopwords
nltk.download('stopwords')

#Importing the data from Kaggle
!pip install kaggle

#creating a folder .kaggle
!mkdir ~/.kaggle

#after uploading the kaggle json file, copy it to the kaggle directory
!cp drive/"My Drive"/kaggle.json ~/.kaggle/

#json file visible
!chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d PromptCloudHQ/amazon-reviews-unlocked-mobile-phones

!unzip amazon-reviews-unlocked-mobile-phones.zip

#saving the data
with open("Amazon_Unlocked_Mobile.csv") as csv_file:
  csv_reader = csv.reader(csv_file)
  colnames = next(csv_reader)
  data = list(csv_reader)

#printing one sample point to see how it is saved like
print(random.sample(data,1))
#each entry consists of product name, brand, price (string), rating (in string), review, review votes.

"""# **Data Cleaning and Preprocessing - Overall Sentiment**"""

x=re.sub("[^a-zA-Z0-9\s]", "", re.sub("[,.&:-]"," ","matu6738,at&t,,3"))
print(x)

x.split()

#extracting initial reviews and ratings from the original data 
initial_reviews = []
ratings = []
review_vote = []  #could be useful later 

for x in data:
  ratings.append(int(x[3]))
  initial_reviews.append(re.sub("[^a-zA-Z0-9\s]", "", re.sub("[,&.:-]"," ",x[4].lower())))
  review_vote.append(x[5])

clean_vote = []
for x in review_vote:
  if x != "":
    clean_vote.append(int(x))
  else: 
    clean_vote.append(0)

print(random.sample(initial_reviews,10))
print(random.sample(ratings,10))
print(random.sample(clean_vote,10))
print(random.sample(review_vote,10))

"""***The Ratings:*** </br>

I will use the ratings as a prediction for the general sentiment for the analysis. There are three classes: Positive, negative and neutral. To increase the accuracy of the model, I will consider positive reviews to have a rating of 4-5, neutral reviews to have a rating of 3, and negative reviews to have a rating of 1-2.

Class 0 := ratings 1-2 </br>
Class 1 := ratings 3 </br>
Class 2 := ratings 4-5
"""

review_class = []

for x in ratings:
  if x == 1 or x == 2:
    review_class.append(0)
  elif x ==3:
    review_class.append(1)
  else:
    review_class.append(2)

"""***The Reviews:*** </br>"""

#the count of top reviews is 123101 out of the 400,000 ones we have (reviews that have been voted for at least once)
count_top_reviews = 0
for x in clean_vote:
  if x>0:
    count_top_reviews+=1
print(count_top_reviews)

stop_words = set(stopwords.words('english'))

#Getting the Vocab:

initial_vocab = []
for i in range(len(clean_vote)):
    list_of_words = initial_reviews[i].split() #get the list of words in each review
    for word in list_of_words:
      initial_vocab.append(word) #add the words to our vocab
      #if (clean_vote[i] != 0): #add the words again to emphasize upvoted reviews
       # for x in range(clean_vote[i]):
        #  initial_vocab.append(word)

filtered_vocab= []
numbers = [str(i) for i in range(0,10)]
for w in initial_vocab: 
    if w not in stop_words and w not in numbers and len(w)>1: 
        filtered_vocab.append(w)

  

#print(len(list(set(filtered_vocab))))
#print(len(list(set(initial_vocab))))

#Identify the most common features
num_features = 4000
vocab_with_count = Counter(filtered_vocab).most_common(num_features)
#print(vocab_with_count[3900:])
vocab = [ word for word, word_count in Counter(filtered_vocab).most_common(num_features)]

print(vocab_with_count[:250])

"""**Taking Samples**"""

sample_review = []
sample_ratings = []
sample_votes = []

for i in random.sample(range(len(initial_reviews)), 100000): #taking 10,000 random sample points
  sample_review.append(initial_reviews[i])
  sample_ratings.append(review_class[i])
  sample_votes.append(clean_vote[i])

for i in random.sample(range(len(sample_review)), 2): #taking 10,000 random sample points
  print(sample_review[i])
  print(sample_ratings[i])

"""# ***Bag of Words***"""

def vectorize(some_string, vocab):
  ### Answer starts here ###
  binary_sentence = []
  review_as_list = some_string.split()
  for word in vocab:
    if word in review_as_list:
      binary_sentence.append(1)
    else:
      binary_sentence.append(0)

  return binary_sentence

vectorized_sample_reviews = [vectorize(review,vocab) for review in sample_review]

print(random.sample(vectorized_sample_reviews, 10))

"""**`Test and Training Data `**"""

X_train = vectorized_sample_reviews[:int(0.8*len(vectorized_sample_reviews))]
y_train = sample_ratings[:int(0.8*len(sample_ratings))]
X_test = vectorized_sample_reviews[int(0.8*len(vectorized_sample_reviews)):]
y_test = sample_ratings[int(0.8*len(sample_ratings)):]

print(random.sample(y_train, 1))

"""# ***SVM***"""

svm_clf = LinearSVC()
svm_clf.fit(X_train, y_train)

svm_y_pred_train = []
svm_y_pred_test = []

for i in range(len(X_train)):
  svm_y_pred_train.append(svm_clf.predict([X_train[i]]))

for i in range(len(X_test)):
  svm_y_pred_test.append(svm_clf.predict([X_test[i]]))

print(metrics.classification_report(y_train, svm_y_pred_train, digits=3))
print(metrics.classification_report(y_test, svm_y_pred_test, digits=3))

y=re.sub("[^a-zA-Z0-9\s]", "", re.sub("[,.&:-]"," ","the battery life is bad, but the phone is good overall"))


print(svm_clf.predict([vectorize(y, vocab)]))

x = 'g'
if (x in vocab):
  print ('y')
  print(vocab.index(x))
else:
  print('n')


vocab[3550]

"""# ***Naive Bayes***"""

count_of_class_zero = 0
count_of_class_one = 0
count_of_class_two = 0

for e in y_train:
  if e == 0:
    count_of_class_zero +=1
  elif e == 1:
    count_of_class_one +=1
  else:
    count_of_class_two +=1


prob_of_class_zero = count_of_class_zero/len(y_train)
prob_of_class_one = count_of_class_one/len(y_train)
prob_of_class_two = 1 - prob_of_class_zero - prob_of_class_one

#vector that contains the probability of each feature given class zero/one
x_zero_class_zero = []
x_zero_class_one= []
x_zero_class_two= []

x_one_class_zero = []
x_one_class_one = []
x_one_class_two = []

for i in range(num_features):
  count_x_zero_class_zero = 0
  count_x_zero_class_one = 0
  count_x_zero_class_two = 0

  count_x_one_class_zero  = 0
  count_x_one_class_one = 0
  count_x_one_class_two = 0
  
  for j in range(len(X_train)):
    if (X_train[j][i] == 0 and y_train[j] == 0):
      count_x_zero_class_zero +=1
    elif (X_train[j][i] == 0 and y_train[j] == 1):
      count_x_zero_class_one +=1
    elif (X_train[j][i] == 0 and y_train[j] == 2):
      count_x_zero_class_two +=1

    elif (X_train[j][i] == 1 and y_train[j] == 0):
      count_x_one_class_zero +=1
    elif (X_train[j][i] == 1 and y_train[j] == 1):
      count_x_one_class_one +=1
    else:
      count_x_one_class_two+=1
  x_zero_class_zero.append(count_x_zero_class_zero/count_of_class_zero)
  x_zero_class_one.append(count_x_zero_class_one/count_of_class_one)
  x_zero_class_two.append(count_x_zero_class_two/count_of_class_two)


  x_one_class_zero.append(count_x_one_class_zero/count_of_class_zero)
  x_one_class_one.append(count_x_one_class_one/count_of_class_one)
  x_one_class_two.append(count_x_one_class_two/count_of_class_two)

def naive_bayes(vec):
  product_given_class_zero = 1
  product_given_class_one = 1
  product_given_class_two = 1

  for i in range(len(vec)):
    if vec[i] ==0 :
      product_given_class_zero*=x_zero_class_zero[i]
      product_given_class_one*=x_zero_class_one[i]
      product_given_class_two*=x_zero_class_two[i]
    else:
      product_given_class_zero*=x_one_class_zero[i]
      product_given_class_one*=x_one_class_one[i]
      product_given_class_two*=x_one_class_two[i]
  
  max_prob = max(product_given_class_zero*prob_of_class_zero, product_given_class_one*prob_of_class_one, product_given_class_two*prob_of_class_two)

  if (max_prob == product_given_class_zero*prob_of_class_zero):
    return 0
  elif (max_prob == product_given_class_one*prob_of_class_one):
    return 1
  else:
    return 2

naive_y_pred_train = []
naive_y_pred_test = []

for i in range(len(X_train)):
  naive_y_pred_train.append(naive_bayes(X_train[i]))

for i in range(len(X_test)):
  naive_y_pred_test.append(naive_bayes(X_test[i]))

print(metrics.classification_report(y_train, naive_y_pred_train, digits=3))
print(metrics.classification_report(y_test, naive_y_pred_test, digits=3))

print(naive_bayes(vectorize(' looks expensive ', vocab)))

"""# ***RFC***"""

rfc = RandomForestClassifier(n_estimators=30, max_depth=18, min_samples_split=4)
rfc.fit(X_train, y_train)

rfc_y_pred_train = []
rfc_y_pred_test = []

for i in range(len(X_train)):
  rfc_y_pred_train.append(rfc.predict([X_train[i]]))

for i in range(len(X_test)):
  rfc_y_pred_test.append(rfc.predict([X_test[i]]))

print(metrics.classification_report(y_train, rfc_y_pred_train, digits=3))
print(metrics.classification_report(y_test, rfc_y_pred_test, digits=3))

print(rfc.predict([vectorize(' expensive', vocab)]))